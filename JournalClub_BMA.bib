@article{PENROSE.1985,
 author = {PENROSE, K. W. and NELSON, A. G. and FISHER, A. G.},
 year = {1985},
 title = {{G}{E}{N}{E}{R}{A}{L}{I}{Z}{E}{D} {B}{O}{D}{Y}-{C}{O}{M}{P}{O}{S}{I}{T}{I}{O}{N} {P}{R}{E}{D}{I}{C}{T}{I}{O}{N} {E}{Q}{U}{A}{T}{I}{O}{N} {F}{O}{R} {M}{E}{N} {U}{S}{I}{N}{G} {S}{I}{M}{P}{L}{E} {M}{E}{A}{S}{U}{R}{E}{M}{E}{N}{T} {T}{E}{C}{H}{N}{I}{Q}{U}{E}{S}},
 pages = {189-189},
 volume = {17},
 number = {2},
 journal = {MEDICINE AND SCIENCE IN SPORTS AND EXERCISE}
}


@article{Madigan.1995,
 author = {Madigan, D. and YORK, J.},
 year = {1995},
 title = {{B}{A}{Y}{E}{S}{I}{A}{N} {G}{R}{A}{P}{H}{I}{C}{A}{L} {M}{O}{D}{E}{L}{S} {F}{O}{R} {D}{I}{S}{C}{R}{E}{T}{E}-{D}{A}{T}{A}},
 pages = {215--232},
 volume = {63},
 number = {2},
 journal = {INTERNATIONAL STATISTICAL REVIEW},
 abstract = {For more than half a century, data analysts have used graphs to represent statistical models, Pn particular, graphical ''conditional independence'' models have emerged as a useful class of models, Applications of such models to probabilistic expert systems, image analysis, and pedigree analysis have motivated much of this work, and several expository texts are now available.

Rather less well known is the development of a Bayesian framework for such models, Expert system applications have motivated this work, where the promise of a model that can update itself as data become available, has generated intense interest from the artificial intelligence community, However, the application to a broader range of data problems has been largely overlooked.

The purpose of this article is to show how Bayesian graphical models unify and simplify many standard discrete data problems such as Bayesian log linear modeling with either complete or incomplete data, closed population estimation, and double sampling, Since conventional model selection fails in these applications, we construct posterior distributions for quantities of interest by averaging across models, Specifically we introduce Markov chain Monte Carlo model composition, a Monte Carlo method for Bayesian model averaging.}
}


@article{Wasserman.2000,
 author = {Wasserman, L.},
 year = {2000},
 title = {{B}ayesian model selection and model averaging},
 pages = {92--107},
 volume = {44},
 number = {1},
 journal = {JOURNAL OF MATHEMATICAL PSYCHOLOGY},
 abstract = {This paper reviews the Bayesian approach to model selection and model averaging. In this review, I emphasize objective Bayesian methods based on noninformative priors. I will also discuss implementation details, approximations. and relationships ro other methods. (C) 2000 Academic Press.}
}


@article{Raftery.1997,
 author = {Raftery, A. E. and Madigan, D. and Hoeting, J. A.},
 year = {1997},
 title = {{B}ayesian model averaging for linear regression models},
 pages = {179--191},
 volume = {92},
 number = {437},
 journal = {JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION},
 abstract = {We consider the problem of accounting for model uncertainty in linear regression models. Conditioning on a single selected model ignores model uncertainty, and thus leads to the underestimation of uncertainty when making inferences about quantities of interest. A Bayesian solution to this problem involves averaging over all possible models (i.e., combinations of predictors) when making inferences about quantities of interest. This approach is often not practical. In this article we offer two alternative approaches. First, we describe an ad hoc procedure, ''Occam's window,'' which indicates a small Set of models over which a model average can be computed. Second, we describe a Markov chain Monte Carlo approach that directly approximates the exact solution. In the presence of model uncertainty, both of these model averaging procedures provide better predictive performance than,any;single model that might reasonably have been selected. In the extreme case where there are many candidate predictors but no relationship between any of them and the response, standard variable selection procedures often choose some subset of variables that yields a high R(2) and a highly significant overall F value. In this situation, Occam's window usually indicates the null model (or a small number of models including the null model) as the only one (or ones) to be considered thus largely resolving the problem of selecting significant models when there is no signal in the data. Software to implement our methods is available from StatLib.}
}


@article{Madigan.1994,
 author = {Madigan, D. and Raftery, A. E.},
 year = {1994},
 title = {{M}{O}{D}{E}{L} {S}{E}{L}{E}{C}{T}{I}{O}{N} {A}{N}{D} {A}{C}{C}{O}{U}{N}{T}{I}{N}{G} {F}{O}{R} {M}{O}{D}{E}{L} {U}{N}{C}{E}{R}{T}{A}{I}{N}{T}{Y} {I}{N} {G}{R}{A}{P}{H}{I}{C}{A}{L} {M}{O}{D}{E}{L}{S} {U}{S}{I}{N}{G} {O}{C}{C}{A}{M}{S} {W}{I}{N}{D}{O}{W}},
 pages = {1535--1546},
 volume = {89},
 number = {428},
 journal = {JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION},
 abstract = {We consider the problem of model selection and accounting for model uncertainty in high-dimensional contingency tables, motivated by expert system applications. The approach most used currently is a stepwise strategy guided by tests based on approximate asymptotic P values leading to the selection of a single model; inference is then conditional on the selected model. The sampling properties of such a strategy are complex, and the failure to take account of model uncertainty leads to underestimation of uncertainty about quantities of interest. In principle, a panacea is provided by the standard Bayesian formalism that averages the posterior distributions of the quantity of interest under each of the models, weighted by their posterior model probabilities. Furthermore, this approach is optimal in the sense of maximizing predictive ability. But this has not been used in practice, because computing the posterior model probabilities is hard and the number of models is very large (often greater than 10(11)). We argue that the standard Bayesian formalism is unsatisfactory and propose an alternative Bayesian approach that, we contend, takes full account of the true model uncertainty by averaging over a much smaller set of models. An efficient search algorithm is developed for finding these models. We consider two classes of graphical models that arise in expert systems: the recursive causal models and the decomposable log-linear models. For each of these, we develop efficient ways of computing exact Bayes factors and hence posterior model probabilities. For the decomposable log-linear models, this is based on properties of chordal graphs and hyper-Markov prior distributions and the resultant calculations can be carried out locally. The end product is an overall strategy for model selection and accounting for model uncertainty that searches efficiently through the very large classes of models involved.

Three examples are given. The first two concern data sets that have been analyzed by several authors in the context of model selection. The third addresses a urological diagnostic problem. In each example, our model averaging approach provides better out-of-sample predictive performance than any single model that might reasonably have been selected.}
}


@article{Daimon.2011,
 author = {Daimon, T. and Zohar, S. and O'Quigley, J.},
 year = {2011},
 title = {{P}osterior maximization and averaging for {B}ayesian working model choice in the continual reassessment method},
 pages = {1563--1573},
 volume = {30},
 number = {13},
 journal = {STATISTICS IN MEDICINE},
 abstract = {The continual reassessment method (CRM) is a method for estimating the maximum tolerated dose in a dose-finding study. Traditionally, use is made of a single working model or 'skeleton' idealizing an underlying true dose-toxicity relationship. This working model is chosen either by discussion with investigators or published data, before the beginning of the trial or simply on the basis of operating characteristics. To overcome the arbitrariness of the choice of such a single working model, Yin and Yuan (J. Am. Statist. Assoc. 2009; 104:954-968) propose a model averaging over a set of working models. Here, instead of averaging, we investigate some alternative Bayesian model criteria that maximize the posterior distribution. We propose three adaptive model-selecting CRMs using the Bayesian model selection criteria, in which we specify in advance a collection of candidate working models for the dose-toxicity relationship, especially initial guesses of toxicity probabilities, and adaptively select the only one working model among the candidates updated by using the original CRM for each working model, based on the posterior model probability, the posterior predictive loss or the deviance information criteria, during the course of the trial. These approaches were compared via a simulation study with the model averaging approach. Copyright (C) 2011 John Wiley {\&} Sons, Ltd.}
}


@article{Clyde.1999,
 author = {Clyde, M.},
 year = {1999},
 title = {{B}ayesian model averaging: {A} tutorial - {C}omment},
 pages = {401--417},
 volume = {14},
 number = {4},
 journal = {STATISTICAL SCIENCE}
}


@article{Hoeting.2000,
 author = {Hoeting, J. A. and Madigan, D. and Raftery, A. E. and Volinsky, C. T.},
 year = {2000},
 title = {{B}ayesian model averaging: {A} tutorial (vol 14, pg 382, 1999)},
 pages = {193--195},
 volume = {15},
 number = {3},
 journal = {STATISTICAL SCIENCE}
}


@article{Genell.2010,
 author = {Genell, Anna and Nemes, Szilard and Steineck, Gunnar and Dickman, Paul W.},
 year = {2010},
 title = {{M}odel selection in {M}edical {R}esearch: {A} simulation study comparing {B}ayesian {M}odel {A}veraging and {S}tepwise {R}egression},
 pages = {-},
 volume = {10},
 journal = {BMC MEDICAL RESEARCH METHODOLOGY},
 abstract = {Background: Automatic variable selection methods are usually discouraged in medical research although we believe they might be valuable for studies where subject matter knowledge is limited. Bayesian model averaging may be useful for model selection but only limited attempts to compare it to stepwise regression have been published. We therefore performed a simulation study to compare stepwise regression with Bayesian model averaging.

Methods: We simulated data corresponding to five different data generating processes and thirty different values of the effect size (the parameter estimate divided by its standard error). Each data generating process contained twenty explanatory variables in total and had between zero and two true predictors. Three data generating processes were built of uncorrelated predictor variables while two had a mixture of correlated and uncorrelated variables. We fitted linear regression models to the simulated data. We used Bayesian model averaging and stepwise regression respectively as model selection procedures and compared the estimated selection probabilities.

Results: The estimated probability of not selecting a redundant variable was between 0.99 and 1 for Bayesian model averaging while approximately 0.95 for stepwise regression when the redundant variable was not correlated with a true predictor. These probabilities did not depend on the effect size of the true predictor. In the case of correlation between a redundant variable and a true predictor, the probability of not selecting a redundant variable was 0.95 to 1 for Bayesian model averaging while for stepwise regression it was between 0.7 and 0.9, depending on the effect size of the true predictor. The probability of selecting a true predictor increased as the effect size of the true predictor increased and leveled out at between 0.9 and 1 for stepwise regression, while it leveled out at 1 for Bayesian model averaging.

Conclusions: Our simulation study showed that under the given conditions, Bayesian model averaging had a higher probability of not selecting a redundant variable than stepwise regression and had a similar probability of selecting a true predictor. Medical researchers building regression models with limited subject matter knowledge could thus benefit from using Bayesian model averaging.}
}


